# -*- coding: utf-8 -*-
"""CNN2D - 10 classes

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/195fqBOTBcDazmOhc8hlpj-DPpdMKVwM9
"""

!pip uninstall -y scikit-learn
!pip uninstall -y pandas
!pip uninstall -y pandas_ml

!pip install scikit-learn==0.20.0
!pip install pandas==0.24.0
!pip install pandas_ml

!pip install --upgrade wandb
!wandb login 3192017bec404c8c4f881ace5da7c4d4debf8b93

# Commented out IPython magic to ensure Python compatibility.
try:
#   %tensorflow_version 2.x

except Exception:
  pass

import tensorflow as tf

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt

import os
import numpy as np

import librosa
import librosa.display as dsp

import random as rn
import pandas as pd

from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras import Sequential

from keras import Input
from keras.engine import Model
from keras.utils import to_categorical
from keras.layers import TimeDistributed, Dropout, Bidirectional, GRU, BatchNormalization, Activation, LeakyReLU, \
    LSTM, Flatten, RepeatVector, Permute, Multiply, Conv2D, MaxPooling2D

from sklearn.model_selection import train_test_split

import wandb
from wandb.keras import WandbCallback

!git clone https://github.com/julianmangut/tfg_speech_commands.git

DATA_DIR = 'tfg_speech_commands/audios_10classes/300/'

folder_list = os.listdir(DATA_DIR)

"""**Preprocesamiento**"""

def extract_features_mfccs(name, features_mfccs):

  pad2d = lambda a, i: a[:, 0: i] if a.shape[1] > i else np.hstack((a, np.zeros((a.shape[0],i - a.shape[1]))))
  
  audio, sample_rate = librosa.load(name, sr=None, res_type='kaiser_fast')
  
  mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)

  features_mfccs.append(pad2d(mfccs,40))

features_mfccs = []
digit = []

for folder_name in folder_list:
  DIR_FOLDER = DATA_DIR + folder_name + '/'
  print(DIR_FOLDER)
  for fname in os.listdir(DIR_FOLDER):
    if '.wav' not in fname:
      continue

    extract_features_mfccs(DIR_FOLDER + fname, features_mfccs)

    digit.append(folder_name)

#features_Panda_mfccs = pd.DataFrame(features_mfccs, columns=['feature_mfccs', 'audio_info'])

features_mfccs = np.array(features_mfccs)

folder_list = np.array(folder_list)

count=0
for x in digit:
  value = np.where(folder_list == x)
  digit[count] = value[0][0]
  count=count+1

y = to_categorical(np.array(digit))

x_train, x_test, y_train, y_test = train_test_split(features_mfccs, y, test_size=0.2, random_state = 127, shuffle=True)

x_train = np.array(x_train)
x_train = x_train.reshape(x_train.shape[0],40,40,1)

print("x_train : ", x_train.shape)

x_validation, x_test, y_validation, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state = 80, shuffle=True)

x_validation = np.array(x_validation)
x_test = np.array(x_test)

x_validation = x_validation.reshape(x_validation.shape[0],40,40,1)
x_test = x_test.reshape(x_test.shape[0],40,40,1)

print("x_validation : ", x_validation.shape)
print("x_test : ", x_test.shape)

"""**Modelo y entrenamiento**"""

model = tf.keras.Sequential([
                             tf.keras.layers.Input(shape=x_train[0].shape),
                             #tf.keras.layers.GaussianNoise(0.01, input_shape=(train_X_ex[0].shape)),
                             tf.keras.layers.Conv2D(64, kernel_size=(4, 4), activation='swish', padding='same'),
                             tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
                             tf.keras.layers.Dropout(0.4),
                             tf.keras.layers.Conv2D(128, kernel_size=(4, 4), activation='swish'),
                             tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
                             tf.keras.layers.Dropout(0.4),
                            #  tf.keras.layers.Conv2D(128, kernel_size=(2, 2), activation='relu'),
                            #  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
                            #  tf.keras.layers.Dropout(0.2),
                             #tf.keras.layers.Flatten(),
                             #tf.keras.layers.Dense(32, activation='relu'),
                             tf.keras.layers.GlobalAveragePooling2D(),
                             tf.keras.layers.Dense(10, activation='softmax')

                            # tf.keras.layers.Conv2D(filters=16, kernel_size=2, input_shape=train_X_ex[0].shape, activation='relu'),
                            # tf.keras.layers.MaxPooling2D(pool_size=2),
                            # tf.keras.layers.Dropout(0.2),

                            # tf.keras.layers.Conv2D(filters=32, kernel_size=2, activation='relu'),
                            # tf.keras.layers.MaxPooling2D(pool_size=2),
                            # tf.keras.layers.Dropout(0.2),

                            # tf.keras.layers.Conv2D(filters=64, kernel_size=2, activation='relu'),
                            # tf.keras.layers.MaxPooling2D(pool_size=2),
                            # tf.keras.layers.Dropout(0.2),

                            # tf.keras.layers.Conv2D(filters=128, kernel_size=2, activation='relu'),
                            # tf.keras.layers.MaxPooling2D(pool_size=2),
                            # tf.keras.layers.Dropout(0.2),
                            # tf.keras.layers.GlobalAveragePooling2D(),

                            # tf.keras.layers.Dense(10, activation='softmax')
])

opt = tf.keras.optimizers.Adam(learning_rate=0.002)

model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

model.summary()

score = model.evaluate(x_test, y_test, verbose=0)
accuracy = 100*score[1]

print("Pre-training accuracy: %.4f%%" % accuracy)

#wandb.init(project="tfg")
#, callbacks=[WandbCallback()]

history = model.fit(x_train, y_train, epochs=100, batch_size=32, verbose=1, validation_data=(x_test, y_test))

plt.plot(history.history['accuracy'],label='Train Accuracy')
plt.plot(history.history['val_accuracy'],label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.plot(history.history['loss'],label='Loss')
plt.plot(history.history['val_loss'],label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

score = model.evaluate(x_train, y_train, verbose=0)
print("Training Accuracy: {0:.2%}".format(score[1]))

score = model.evaluate(x_test, y_test, verbose=0)
print("Testing Accuracy: {0:.2%}".format(score[1]))

score = model.evaluate(x_validation, y_validation, verbose=0)
print("Validation Accuracy: {0:.2%}".format(score[1]))

"""**Creación Matriz de Confusión y estadísticas**"""

prediction = model.predict_classes(x_validation)
value = []

for i in range(0,1000):
  value.append(np.where(y_validation[i] == 1)[0][0])

from sklearn.metrics import confusion_matrix
import seaborn as sns
from pandas_ml import ConfusionMatrix as confusion_pandas

labels=[0,1,2,3,4,5,6,7,8,9]
figsize=(10,10)

print(value)
print(prediction)

cm = confusion_matrix(value, prediction, labels=labels)
cm_sum = np.sum(cm, axis=1, keepdims=True)
cm_perc = cm / cm_sum.astype(float) * 100
annot = np.empty_like(cm).astype(str)
nrows, ncols = cm.shape
for i in range(nrows):
    for j in range(ncols):
        c = cm[i, j]
        p = cm_perc[i, j]
        if i == j:
            s = cm_sum[i]
            annot[i, j] = '%.1f%%\n%d/%d' % (p, c, s)
        elif c == 0:
            annot[i, j] = '0'
        else:
            annot[i, j] = '%.1f%%\n%d' % (p, c)
cm = pd.DataFrame(cm, index=labels, columns=labels)
cm.index.name = 'Actual'
cm.columns.name = 'Predicted'
fig, ax = plt.subplots(figsize=figsize)
sns.heatmap(cm, annot=annot, fmt='', ax=ax, cmap="Greens")

confusion_pandas = confusion_pandas(value, prediction)
confusion_pandas.print_stats()

print("TPR")
print(confusion_pandas.TPR)
print("PPV")
print(confusion_pandas.PPV)
print("F1")
print(confusion_pandas.F1_score)

"""**Transformación a Tensorflow Lite**"""

import time

t = time.time()

export_path_sm = "./{}".format(int(t))
print(export_path_sm)

tf.saved_model.save(model, export_path_sm)

# Convert the model.
converter = tf.lite.TFLiteConverter.from_saved_model(export_path_sm)
tflite_model = converter.convert()

import pathlib

tflite_model_file = pathlib.Path('CNN2D_10Classes_1000.tflite')
tflite_model_file.write_bytes(tflite_model)

try:
  from google.colab import files
  files.download(tflite_model_file)
except:
  pass