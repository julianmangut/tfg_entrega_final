# -*- coding: utf-8 -*-
"""Multicapa - Yes/No MFCCS

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qrp1zK0Fei6tcPVEPGkRJeA6qY4pqBlH
"""

!pip uninstall -y scikit-learn
!pip uninstall -y pandas
!pip uninstall -y pandas_ml

!pip install scikit-learn==0.20.0
!pip install pandas==0.24.0
!pip install pandas_ml

!pip install --upgrade wandb
!wandb login 3192017bec404c8c4f881ace5da7c4d4debf8b93

# Commented out IPython magic to ensure Python compatibility.
try:
#   %tensorflow_version 2.x
except Exception:
  pass

import tensorflow as tf

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt

import os

import numpy as np

import librosa
import librosa.display as dsp

import pandas as pd
#import pandas_ml

from tensorflow.python.keras import Sequential
from keras import Input
from keras.engine import Model
from keras.utils import to_categorical
from keras.layers import TimeDistributed, Dropout, Bidirectional, GRU, BatchNormalization, Activation, LeakyReLU, \
    LSTM, Flatten, RepeatVector, Permute, Multiply, Conv2D, MaxPooling2D, Dense

from sklearn.model_selection import train_test_split

# import wandb
# from wandb.keras import WandbCallback

!git clone https://github.com/julianmangut/tfg_speech_commands.git

DATA_DIR_GENERAL = 'tfg_speech_commands/audios/300/'

folder_list = os.listdir(DATA_DIR_GENERAL)

"""**Preprocesamiento**"""

def extract_features_mfccs(wav, sample_rate):
  mfccs = librosa.feature.mfcc(y=wav, sr=sample_rate, n_mfcc=50)
  mfccs_processed = np.mean(mfccs.T, axis=0)

  return mfccs_processed

features_mfccs = []

for folder_name in folder_list:
  DIR_FOLDER = DATA_DIR_GENERAL + folder_name + '/'
  print(DIR_FOLDER)
  if(folder_name == 'no'):
    number = 0
  else:
    number = 1
  for fname in os.listdir(DIR_FOLDER):
    if '.wav' not in fname:
      continue

    audio, sample_rate = librosa.load(DIR_FOLDER + fname, sr=None, res_type='kaiser_fast')

    data_mfccs = extract_features_mfccs(audio, sample_rate)

    features_mfccs.append([data_mfccs, number])

features_Panda_mfccs = pd.DataFrame(features_mfccs, columns=['feature_mfccs', 'audio_info'])

features_Panda_mfccs.head()

X_mfccs = np.array(features_Panda_mfccs.feature_mfccs.tolist())
y = np.array(features_Panda_mfccs.audio_info.tolist())

yy = to_categorical(y)

x_train_mfccs, x_test_mfccs, y_train_mfccs, y_test_mfccs = train_test_split(X_mfccs, yy, test_size=0.2, random_state = 127)
print("X_train shape : ", x_train_mfccs.shape)

x_validation_mfccs, x_test_mfccs, y_validation_mfccs, y_test_mfccs = train_test_split(x_test_mfccs, y_test_mfccs, test_size=0.5, random_state=80)
print("X_test shape : ", x_test_mfccs.shape)
print("X_validation shape : ",x_validation_mfccs.shape)

"""**Modelo y entrenamiento**"""

num_labels = yy.shape[1]

model_mfccs = tf.keras.Sequential([
                             tf.keras.layers.Input(shape=(50,)),
                             tf.keras.layers.Dense(256, activation='swish'),
                             tf.keras.layers.Dropout(0.5),
                             tf.keras.layers.Dense(256, activation='swish'),
                             tf.keras.layers.Dropout(0.5),
                             tf.keras.layers.Dense(num_labels, activation='softmax')
])

model_mfccs.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')

model_mfccs.summary()

score = model_mfccs.evaluate(x_test_mfccs, y_test_mfccs, verbose=0)
accuracy = 100*score[1]

print("Pre-training accuracy: %.4f%%" % accuracy)

num_epochs = 100
num_batch_size = 32

#wandb.init(project="tfg")
#, callbacks=[WandbCallback()]

history = model_mfccs.fit(x_train_mfccs, y_train_mfccs, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_validation_mfccs, y_validation_mfccs), verbose=1)

score = model_mfccs.evaluate(x_train_mfccs, y_train_mfccs, verbose=0)
print("Training Accuracy: {0:.2%}".format(score[1]))

score = model_mfccs.evaluate(x_test_mfccs, y_test_mfccs, verbose=0)
print("Testing Accuracy: {0:.2%}".format(score[1]))

score = model_mfccs.evaluate(x_validation_mfccs, y_validation_mfccs, verbose=0)
print("Validation Accuracy: {0:.2%}".format(score[1]))

"""**Estadísticas y generación de Matriz de Confusión**"""

prediction = model_mfccs.predict_classes(x_validation_mfccs)
value = []

for i in range(0,200):
  value.append(np.where(y_validation_mfccs[i] == 1)[0][0])

from sklearn.metrics import confusion_matrix
import seaborn as sns
from pandas_ml import ConfusionMatrix as confusion_pandas

labels=[0,1]
figsize=(2,2)

cm = confusion_matrix(value, prediction, labels=labels)
cm_sum = np.sum(cm, axis=1, keepdims=True)
cm_perc = cm / cm_sum.astype(float) * 100
annot = np.empty_like(cm).astype(str)
nrows, ncols = cm.shape
for i in range(nrows):
    for j in range(ncols):
        c = cm[i, j]
        p = cm_perc[i, j]
        if i == j:
            s = cm_sum[i]
            annot[i, j] = '%.1f%%\n%d/%d' % (p, c, s)
        elif c == 0:
            annot[i, j] = '0'
        else:
            annot[i, j] = '%.1f%%\n%d' % (p, c)
cm = pd.DataFrame(cm, index=labels, columns=labels)
cm.index.name = 'Actual'
cm.columns.name = 'Predicted'
fig, ax = plt.subplots(figsize=figsize)
sns.heatmap(cm, annot=annot, fmt='', ax=ax, cmap="Greens")

confusion_pandas = confusion_pandas(value, prediction)
confusion_pandas.print_stats()

print("TPR")
print(confusion_pandas.TPR)
print("PPV")
print(confusion_pandas.PPV)
print("F1")
print(confusion_pandas.F1_score)